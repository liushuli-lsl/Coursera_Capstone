{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Sentiment Analysis and Rating Prediction From The Review Text"}, {"metadata": {}, "cell_type": "markdown", "source": "Sentiment ananlysis and rating prediction are among the imporant machine learning topics that help companies find if the users are happy or unhappy with the service/product provided. The users write reviews of the products/services on various platforms, such as social networking websites like Facebook and Twitter, Blogs, and service offering websites. The ananlysis of such reviews to find the coustomer satisfaction will be helpful for companies to improve their products as well as the customer service.\n\nIn this project, I aim to build a machine learning system that will predict the user rating from his text review. Precisely, I will work on building the models for the following.\n\n1. Predict the users' sentiments (positive or negative).\n2. Predict his product/service rating on a scale of 1 to 5.\n\nI have already done the ETL in the other notebook. So here, I will just load the data prepare it for the model training, valiadation and testing and describe the deep learning model employed. In this notebook, I will just focus on the rating predcitction, i.e. mutliclass-classfication. For sentiment analysis, i.e. the binary classification, I have described the modeling in another notebook."}, {"metadata": {}, "cell_type": "markdown", "source": "So, let's just start with loading the required libraries."}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nimport gzip\nimport glob\nimport os\nimport re", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, GRU, Convolution1D, Flatten, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n", "execution_count": 3, "outputs": [{"output_type": "stream", "text": "Using TensorFlow backend.\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Loading the data from the CSV"}, {"metadata": {}, "cell_type": "code", "source": "df_ReviewRating = pd.read_csv('AmazonBookReviews_Ratings.csv')\ndf_ReviewRating.shape", "execution_count": 4, "outputs": [{"output_type": "execute_result", "execution_count": 4, "data": {"text/plain": "(68563, 3)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "lenthsStr = df_ReviewRating['reviewText'].apply(str).map(len)\nmaxlength = max(lenthsStr)\nmaxindex = lenthsStr[lenthsStr == maxlength].index[0]\nprint('The length of longest review text: '+ str(maxlength))\nprint('The index of review with maximum length: '+ str(maxindex))", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "The length of longest review text: 499\nThe index of review with maximum length: 844\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Spliting Data Set into Train, Validation and Test"}, {"metadata": {}, "cell_type": "code", "source": "X_train, X_test, y_train, y_test = train_test_split(df_ReviewRating['reviewText'], df_ReviewRating['overall'], test_size=0.2, random_state=1)", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(X_train.shape)\nprint(X_train[0:5])\nprint(y_train.shape)\nprint(y_train[0:5])\n\ndf_train = pd.concat([X_train, y_train], axis=1)\ndf_train.groupby('overall').count()/df_train.shape[0]*100", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "(43880,)\n11362    This must be the worst series I have read by a...\n21675    I can't stand all of this female inner drama i...\n9518     She is a woman I have had admiration for. Her ...\n13030    There's nothing more festive than Christmas in...\n65150    I started with book one, I Love My Breakup bec...\nName: reviewText, dtype: object\n(43880,)\n11362    2.0\n21675    4.0\n9518     3.0\n13030    5.0\n65150    5.0\nName: overall, dtype: float64\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "         reviewText\noverall            \n1.0        3.099362\n2.0        3.719234\n3.0        9.307201\n4.0       23.144941\n5.0       60.729262", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewText</th>\n    </tr>\n    <tr>\n      <th>overall</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1.0</th>\n      <td>3.099362</td>\n    </tr>\n    <tr>\n      <th>2.0</th>\n      <td>3.719234</td>\n    </tr>\n    <tr>\n      <th>3.0</th>\n      <td>9.307201</td>\n    </tr>\n    <tr>\n      <th>4.0</th>\n      <td>23.144941</td>\n    </tr>\n    <tr>\n      <th>5.0</th>\n      <td>60.729262</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "print(X_test.shape)\nprint(X_test[0:5])\nprint(y_test.shape)\nprint(y_test[0:5])\ndf_test = pd.concat([X_test, y_test], axis=1)\ndf_test.groupby('overall').count()/df_test.shape[0]*100", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "(13713,)\n15006    I love this series!!!! If you haven't read it ...\n28600    In my opinion, Jessica Gibson had a good story...\n60398    I actually liked the first book better. I hate...\n43146    It was another example of why I like reading H...\n51397    The story is a bit ridiculous at times but alw...\nName: reviewText, dtype: object\n(13713,)\n15006    5.0\n28600    2.0\n60398    3.0\n43146    5.0\n51397    3.0\nName: overall, dtype: float64\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 9, "data": {"text/plain": "         reviewText\noverall            \n1.0        3.223219\n2.0        3.981623\n3.0        9.319624\n4.0       22.577117\n5.0       60.898418", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewText</th>\n    </tr>\n    <tr>\n      <th>overall</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1.0</th>\n      <td>3.223219</td>\n    </tr>\n    <tr>\n      <th>2.0</th>\n      <td>3.981623</td>\n    </tr>\n    <tr>\n      <th>3.0</th>\n      <td>9.319624</td>\n    </tr>\n    <tr>\n      <th>4.0</th>\n      <td>22.577117</td>\n    </tr>\n    <tr>\n      <th>5.0</th>\n      <td>60.898418</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "print(X_val.shape)\nprint(X_val[0:5])\nprint(y_val.shape)\nprint(y_val[0:5])\ndf_val = pd.concat([X_val, y_val], axis=1)\ndf_val.groupby('overall').count()/df_val.shape[0]*100", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "(10970,)\n20329    I truly loved this story and the author manage...\n65932    This part here damn. I must say it will cause ...\n4703     This is a concise and direct read on how to st...\n65888    I really like the information provided in this...\n15641    I love this entire series so far...characters ...\nName: reviewText, dtype: object\n(10970,)\n20329    5.0\n65932    5.0\n4703     4.0\n65888    4.0\n15641    5.0\nName: overall, dtype: float64\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "         reviewText\noverall            \n1.0        3.354603\n2.0        3.956244\n3.0        9.908842\n4.0       22.260711\n5.0       60.519599", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewText</th>\n    </tr>\n    <tr>\n      <th>overall</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1.0</th>\n      <td>3.354603</td>\n    </tr>\n    <tr>\n      <th>2.0</th>\n      <td>3.956244</td>\n    </tr>\n    <tr>\n      <th>3.0</th>\n      <td>9.908842</td>\n    </tr>\n    <tr>\n      <th>4.0</th>\n      <td>22.260711</td>\n    </tr>\n    <tr>\n      <th>5.0</th>\n      <td>60.519599</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "We see that the distribution of classess in train, validation and test set is representative of the resal data set."}, {"metadata": {}, "cell_type": "markdown", "source": "## Data Preparation  for Modeling"}, {"metadata": {}, "cell_type": "markdown", "source": "In this step, we first tokenize the textual data into words and convert it into sequences of same length."}, {"metadata": {}, "cell_type": "markdown", "source": "### Tokenization"}, {"metadata": {}, "cell_type": "code", "source": "%%time\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_ReviewRating['reviewText'])\n", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "CPU times: user 2.71 s, sys: 52 ms, total: 2.76 s\nWall time: 2.84 s\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "vocab_size = len(tokenizer.word_index) + 1\nvocab_size", "execution_count": 12, "outputs": [{"output_type": "execute_result", "execution_count": 12, "data": {"text/plain": "46363"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "%%time\nsequence_train = tokenizer.texts_to_sequences(X_train)\nsequence_test = tokenizer.texts_to_sequences(X_test)\nsequence_val = tokenizer.texts_to_sequences(X_val)", "execution_count": 13, "outputs": [{"output_type": "stream", "text": "CPU times: user 2.17 s, sys: 84 ms, total: 2.25 s\nWall time: 2.33 s\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "%%time\nX_train_pad = pad_sequences(sequence_train, maxlen=maxlength)\nX_test_pad = pad_sequences(sequence_test, maxlen=maxlength)\nX_val_pad = pad_sequences(sequence_val, maxlen=maxlength)", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "CPU times: user 464 ms, sys: 308 ms, total: 772 ms\nWall time: 802 ms\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Let us convert the labels vector to a matrix (one-hot encoded) using to_categorical. The to_categorical expects the label to start from 0, so I changed the labels. Now, rating one is represented by 0 and five by 4."}, {"metadata": {}, "cell_type": "code", "source": "y_train_label = to_categorical(np.asarray(y_train - 1))\nprint(y_train[0:5])\ny_test_label = to_categorical(np.asarray(y_test - 1))\nprint(y_test[0:5])\ny_val_label = to_categorical(np.asarray(y_val - 1))\nprint(y_val[0:5])", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "11362    2.0\n21675    4.0\n9518     3.0\n13030    5.0\n65150    5.0\nName: overall, dtype: float64\n15006    5.0\n28600    2.0\n60398    3.0\n43146    5.0\n51397    3.0\nName: overall, dtype: float64\n20329    5.0\n65932    5.0\n4703     4.0\n65888    4.0\n15641    5.0\nName: overall, dtype: float64\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "y_train_label[0:5]", "execution_count": 16, "outputs": [{"output_type": "execute_result", "execution_count": 16, "data": {"text/plain": "array([[0., 1., 0., 0., 0.],\n       [0., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 1.],\n       [0., 0., 0., 0., 1.]], dtype=float32)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Model Training and Evaluation"}, {"metadata": {}, "cell_type": "markdown", "source": "In this project, I decided to use three different deep learning models. The models and their evaluation follows."}, {"metadata": {}, "cell_type": "markdown", "source": "### GRU based Model"}, {"metadata": {}, "cell_type": "code", "source": "# embedding_dimensions =  vocab_size**0.25\nembedding_dimensions = 100", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Model definition ..."}, {"metadata": {}, "cell_type": "code", "source": "model = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dimensions, input_length=maxlength))\nmodel.add(GRU(units=32, dropout=0.2))\nmodel.add(Dense(5, activation='softmax'))\nprint(model.summary()) ", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 499, 100)          4636300   \n_________________________________________________________________\ngru_1 (GRU)                  (None, 32)                12768     \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 165       \n=================================================================\nTotal params: 4,649,233\nTrainable params: 4,649,233\nNon-trainable params: 0\n_________________________________________________________________\nNone\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Model compilation and training ..."}, {"metadata": {}, "cell_type": "code", "source": "%%time\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "CPU times: user 44 ms, sys: 8 ms, total: 52 ms\nWall time: 61.9 ms\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "%%time\nmodel.fit(X_train_pad, y_train_label,\n          batch_size=128,\n          epochs=5,\n          validation_data=(X_val_pad, y_val_label),\n          callbacks = [checkpoint, early_stop])", "execution_count": 20, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'checkpoint' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n", "\u001b[0;31mNameError\u001b[0m: name 'checkpoint' is not defined"]}]}, {"metadata": {}, "cell_type": "markdown", "source": "Model evaluation ..."}, {"metadata": {}, "cell_type": "code", "source": "scores = model.evaluate(X_test_pad, y_test_label, verbose=0) \nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "Accuracy: 18.75%\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### LSTM basedModel"}, {"metadata": {}, "cell_type": "markdown", "source": "Model definition ..."}, {"metadata": {}, "cell_type": "code", "source": "model = Sequential() \nmodel.add(Embedding(vocab_size, embedding_dimensions, input_length=maxlength)) \nmodel.add(LSTM(100)) \nmodel.add(Dense(5, activation='softmax'))\nprint(model.summary()) ", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 499, 100)          4636300   \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 100)               80400     \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 505       \n=================================================================\nTotal params: 4,717,205\nTrainable params: 4,717,205\nNon-trainable params: 0\n_________________________________________________________________\nNone\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Model comiplation and training ..."}, {"metadata": {}, "cell_type": "code", "source": "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])", "execution_count": 23, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%%time\nmodel.fit(X_train_pad, y_train_label,\n          batch_size=128,\n          epochs=5,\n          validation_data=(X_val_pad, y_val_label))", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "WARNING:tensorflow:From /opt/conda/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\nTrain on 43880 samples, validate on 10970 samples\nEpoch 1/5\n43880/43880 [==============================] - 5369s 122ms/step - loss: 0.9189 - acc: 0.6324 - val_loss: 0.8127 - val_acc: 0.6637\nEpoch 2/5\n43880/43880 [==============================] - 5350s 122ms/step - loss: 0.7262 - acc: 0.6962 - val_loss: 0.7879 - val_acc: 0.6701\nEpoch 3/5\n43880/43880 [==============================] - 5561s 127ms/step - loss: 0.6297 - acc: 0.7429 - val_loss: 0.8425 - val_acc: 0.6572\nEpoch 4/5\n43880/43880 [==============================] - 5667s 129ms/step - loss: 0.5408 - acc: 0.7877 - val_loss: 0.9478 - val_acc: 0.6559\nEpoch 5/5\n43880/43880 [==============================] - 5686s 130ms/step - loss: 0.4614 - acc: 0.8226 - val_loss: 0.9885 - val_acc: 0.6424\nCPU times: user 2h 36min 53s, sys: 1h 37min 2s, total: 4h 13min 56s\nWall time: 7h 40min 33s\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 24, "data": {"text/plain": "<keras.callbacks.History at 0x7fb752c8ab00>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Model evaluation ..."}, {"metadata": {}, "cell_type": "code", "source": "scores = model.evaluate(X_test_pad, y_test_label, verbose=0) \nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "Accuracy: 63.93%\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "### 1D CNN based Model "}, {"metadata": {}, "cell_type": "markdown", "source": "Model definition ..."}, {"metadata": {}, "cell_type": "code", "source": "model = Sequential() \nmodel.add(Embedding(vocab_size, embedding_dimensions, input_length=maxlength)) \nmodel.add(Convolution1D(64, 3, padding='same'))\nmodel.add(Convolution1D(32, 3, padding='same'))\nmodel.add(Convolution1D(16, 3, padding='same'))\nmodel.add(Flatten())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(180,activation='sigmoid'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(5, activation='softmax'))\nprint(model.summary()) ", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 499, 100)          4636300   \n_________________________________________________________________\nconv1d_1 (Conv1D)            (None, 499, 64)           19264     \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 499, 32)           6176      \n_________________________________________________________________\nconv1d_3 (Conv1D)            (None, 499, 16)           1552      \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 7984)              0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 7984)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 180)               1437300   \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 180)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 5)                 905       \n=================================================================\nTotal params: 6,101,497\nTrainable params: 6,101,497\nNon-trainable params: 0\n_________________________________________________________________\nNone\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Model compilation and training ..."}, {"metadata": {}, "cell_type": "code", "source": "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy']) ", "execution_count": 27, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "%%time\nmodel.fit(X_train_pad, y_train_label,\n          batch_size=128,\n          epochs=5,\n          validation_data=(X_val_pad, y_val_label))", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "Train on 43880 samples, validate on 10970 samples\nEpoch 1/5\n43880/43880 [==============================] - 1042s 24ms/step - loss: 0.9132 - acc: 0.6342 - val_loss: 0.8036 - val_acc: 0.6635\nEpoch 2/5\n43880/43880 [==============================] - 992s 23ms/step - loss: 0.7128 - acc: 0.7024 - val_loss: 0.7970 - val_acc: 0.6655\nEpoch 3/5\n43880/43880 [==============================] - 962s 22ms/step - loss: 0.5675 - acc: 0.7750 - val_loss: 0.8970 - val_acc: 0.6516\nEpoch 4/5\n43880/43880 [==============================] - 984s 22ms/step - loss: 0.3690 - acc: 0.8616 - val_loss: 1.0888 - val_acc: 0.6206\nEpoch 5/5\n43880/43880 [==============================] - 965s 22ms/step - loss: 0.2246 - acc: 0.9197 - val_loss: 1.3935 - val_acc: 0.5969\nCPU times: user 1h 56s, sys: 0 ns, total: 1h 56s\nWall time: 1h 22min 25s\n", "name": "stdout"}, {"output_type": "execute_result", "execution_count": 28, "data": {"text/plain": "<keras.callbacks.History at 0x7fb6782085c0>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Model evaluation ..."}, {"metadata": {}, "cell_type": "code", "source": "scores = model.evaluate(X_test_pad, y_test_label, verbose=0) \nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "Accuracy: 59.06%\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Conclusion"}, {"metadata": {}, "cell_type": "markdown", "source": "We have trained three different deep learning models to predict the user ratings from the text of the review they wrote. "}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}